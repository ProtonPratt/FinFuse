{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    published_date                                              title  \\\n",
      "0  20220301T080000  Tesla Rival Slashes 2022 Production Outlook; L...   \n",
      "1  20220301T080000  Nasdaq moves into positive territory as broade...   \n",
      "2  20220301T113014  BYD to use Baidu's autonomous driving technolo...   \n",
      "3  20220301T163019  Chinese electric car makers' February sales de...   \n",
      "4  20220302T080000  Box Stock Jumps As Fourth-Quarter Results, Out...   \n",
      "\n",
      "                                             summary ticker  \\\n",
      "0  Lucid Stock Sinks As Tesla Rival Slashes 2022 ...   TSLA   \n",
      "1  Live updates: Dow tumbles as Russia threatens ...   TSLA   \n",
      "2  BYD, China's biggest electric vehicle (EV) bui...   TSLA   \n",
      "3  China's three biggest makers of smart electric...   TSLA   \n",
      "4  Box Stock Jumps As Earnings Beat Estimates Inv...   TSLA   \n",
      "\n",
      "   ticker_sentiment_score ticker_sentiment_label  \n",
      "0               -0.058479                Neutral  \n",
      "1               -0.101763                Neutral  \n",
      "2                0.124467                Neutral  \n",
      "3                0.066845                Neutral  \n",
      "4                0.036248                Neutral  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your folder\n",
    "folder_path = './news_data'  # Update this if you're running from a different directory\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*_alpha_news_data.csv'))\n",
    "\n",
    "# Read and concatenate all CSVs\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "all_data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(all_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned and saved merged data to ./merged/merged_alpha_news_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Define input and output directories\n",
    "input_dir = './news_data'\n",
    "output_dir = './merged'\n",
    "output_file = 'merged_alpha_news_data.csv'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all relevant CSV files\n",
    "csv_files = glob.glob(os.path.join(input_dir, '*_alpha_news_data.csv'))\n",
    "\n",
    "# Load and merge all CSVs\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Optional: list of columns to clean\n",
    "text_columns = ['title', 'summary']\n",
    "\n",
    "# Clean text columns\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "        text = text.strip()\n",
    "    return text\n",
    "\n",
    "for col in text_columns:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].apply(clean_text)\n",
    "\n",
    "# Save cleaned DataFrame\n",
    "merged_df.to_csv(os.path.join(output_dir, output_file), index=False)\n",
    "\n",
    "print(f\"‚úÖ Cleaned and saved merged data to {os.path.join(output_dir, output_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed NVDA: 20 file(s) saved in ./split/NVDA\n",
      "‚úÖ Processed AMZN: 20 file(s) saved in ./split/AMZN\n",
      "‚úÖ Processed TSLA: 20 file(s) saved in ./split/TSLA\n",
      "‚úÖ Processed NKE: 20 file(s) saved in ./split/NKE\n",
      "‚úÖ Processed AAPL: 20 file(s) saved in ./split/AAPL\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# Input and output paths\n",
    "input_dir = './news_data'\n",
    "output_base = './split'\n",
    "cleaned_dir = './cleaned'\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Process each file separately\n",
    "csv_files = glob.glob(os.path.join(input_dir, '*_alpha_news_data.csv'))\n",
    "\n",
    "for file_path in csv_files:\n",
    "    # Extract ticker name from filename\n",
    "    base_name = os.path.basename(file_path)\n",
    "    ticker = base_name.split('_')[0].upper()  # e.g., AAPL\n",
    "\n",
    "    # Create ticker-specific output directory\n",
    "    ticker_dir = os.path.join(output_base, ticker)\n",
    "    os.makedirs(ticker_dir, exist_ok=True)\n",
    "\n",
    "    # Load and clean the CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    for col in ['title', 'summary']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(clean_text)\n",
    "            \n",
    "    # Save the entire cleaned CSV for this stock\n",
    "    os.makedirs(cleaned_dir, exist_ok=True)\n",
    "    cleaned_csv_path = os.path.join(cleaned_dir, f'{ticker}_alpha_news_data.csv')\n",
    "    df.to_csv(cleaned_csv_path, index=False)\n",
    "\n",
    "    # # Split into 1000-row chunks\n",
    "    # chunk_size = 1000\n",
    "    # total_rows = df.shape[0]\n",
    "    # num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "    # for i in range(num_chunks):\n",
    "    #     start = i * chunk_size\n",
    "    #     end = min(start + chunk_size, total_rows)\n",
    "    #     chunk_df = df.iloc[start:end]\n",
    "\n",
    "    #     # Save each chunk\n",
    "    #     output_file = os.path.join(ticker_dir, f'stock_data_part_{i+1}.csv')\n",
    "    #     chunk_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Processing: ./stocks_data/NVDA_yahoo_data_0.csv\n",
      "‚úÖ Saved cleaned file to: ./stocks_cleaned/NVDA_yahoo_data_0.csv\n",
      "üìÇ Processing: ./stocks_data/AAPL_yahoo_data_0.csv\n",
      "‚úÖ Saved cleaned file to: ./stocks_cleaned/AAPL_yahoo_data_0.csv\n",
      "üìÇ Processing: ./stocks_data/NKE_yahoo_data_0.csv\n",
      "‚úÖ Saved cleaned file to: ./stocks_cleaned/NKE_yahoo_data_0.csv\n",
      "üìÇ Processing: ./stocks_data/AMZN_yahoo_data_0.csv\n",
      "‚úÖ Saved cleaned file to: ./stocks_cleaned/AMZN_yahoo_data_0.csv\n",
      "üìÇ Processing: ./stocks_data/TSLA_yahoo_data_0.csv\n",
      "‚úÖ Saved cleaned file to: ./stocks_cleaned/TSLA_yahoo_data_0.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Paths\n",
    "input_dir = './stocks_data'\n",
    "output_dir = './stocks_cleaned'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file in glob.glob(os.path.join(input_dir, '*.csv')):\n",
    "    print(f\"üìÇ Processing: {file}\")\n",
    "\n",
    "    # Load with headers\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Clean 'Date' column\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce', utc=True)\n",
    "        df['Date'] = df['Date'].dt.tz_convert(None)\n",
    "        df.dropna(subset=['Date'], inplace=True)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipping {file} - no 'Date' column found\")\n",
    "        continue\n",
    "\n",
    "    # Remove empty or unnamed columns\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Sort by date\n",
    "    df.sort_values(by='Date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save cleaned file\n",
    "    out_path = os.path.join(output_dir, os.path.basename(file))\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"‚úÖ Saved cleaned file to: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ins_dec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
